import ssl
import urllib3
from dataclasses import dataclass
from typing import Optional

import feedparser
import pandas as pd
import requests
from googlenewsdecoder import gnewsdecoder
from newspaper import Article, Config

# ============================================================
# ì„¤ì •
# ============================================================
@dataclass
class CrawlerConfig:
    max_total: int = 30
    days: int = 14
    candidates_per_query: int = 5
    min_content_length: int = 150
    request_timeout: int = 15
    user_agent: str = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

EXCLUDE_KEYWORDS = ["ë°°íƒ€ì ", "ì˜ìƒ", "ì¢…ëª©", "ì£¼ê°€", "ê¸‰ë“±", "ê¸‰ë½", "ë§¤ìˆ˜", "ë§¤ë„"]

PRIORITY_KEYWORDS = {
    "ì¶œì‹œ": 10, "ëŸ°ì¹­": 10, "ì˜¤í”ˆ": 8, "ì„œë¹„ìŠ¤": 12,
    "ë°œí‘œ": 6, "ë„ì…": 6, "ê°œë°œ": 5, "ìë™": 10,
    "ì±—ë´‡": 10, "GPT": 10, "ìƒì„±í˜•": 10, "LLM": 10,
    "í”Œë«í¼": 3, "ì†”ë£¨ì…˜": 3, "ì‹œìŠ¤í…œ": 2,
}

SEARCH_CATEGORIES = [
    {
        "category": "ë³´í—˜ì‚¬",
        "queries": [
            "ì‚¼ì„±í™”ì¬", "í˜„ëŒ€í•´ìƒ", "DBì†í•´ë³´í—˜", "KBì†í•´ë³´í—˜", "ë©”ë¦¬ì¸ í™”ì¬", "í† ìŠ¤ì¸ìŠˆì–´ëŸ°ìŠ¤",
            "ì‚¼ì„±ìƒëª…", "êµë³´ìƒëª…", "í•œí™”ìƒëª…", "ì‹ í•œë¼ì´í”„", "NHë†í˜‘ìƒëª…", "KBë¼ì´í”„", "NHë†í˜‘ìƒëª…"
        ]
    },
    {
        "category": "ì€í–‰",
        "queries": ["í† ìŠ¤ë±…í¬", "ìš°ë¦¬ì€í–‰", "êµ­ë¯¼ì€í–‰", "ì‹ í•œì€í–‰", "í•˜ë‚˜ì€í–‰", "ê¸°ì—…ì€í–‰"]
    },
    {
        "category": "Tech",
        "queries": ["êµ¬ê¸€", "OpenAI", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸"]
    },
    {
        "category": "ì¦ê¶Œì‚¬",
        "queries": ["NHíˆ¬ìì¦ê¶Œ", "ë¯¸ë˜ì—ì…‹ì¦ê¶Œ", "í•œêµ­íˆ¬ìì¦ê¶Œ", "ì‚¼ì„±ì¦ê¶Œ", "ì‹ í•œíˆ¬ìì¦ê¶Œ", "KBì¦ê¶Œ", "í‚¤ì›€ì¦ê¶Œ", "í† ìŠ¤ì¦ê¶Œ"]
    },
]


# ============================================================
# SSL ì„¤ì •
# ============================================================
def setup_ssl():
    ssl._create_default_https_context = ssl._create_unverified_context
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    original_request = requests.Session.request
    def patched_request(self, *args, **kwargs):
        kwargs['verify'] = False
        return original_request(self, *args, **kwargs)
    requests.Session.request = patched_request


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def get_rss_url(query: str, days: int) -> str:
    encoded_query = f"{query} AI when:{days}d".replace(" ", "+")
    return f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"


def calculate_score(title: str, content: str) -> int:
    text = f"{title} {content}"
    score = 0
    for keyword, weight in PRIORITY_KEYWORDS.items():
        if keyword in text:
            score += weight * 2 if keyword in title else weight
    return score


def decode_url(link: str) -> str:
    try:
        result = gnewsdecoder(link)
        return result.get('decoded_url', link) if isinstance(result, dict) else result
    except Exception:
        return link


def fetch_article(url: str, config: Config) -> Optional[str]:
    """ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜´. ì‹¤íŒ¨ì‹œ None ë°˜í™˜"""
    try:
        article = Article(url, language='ko', config=config)
        article.download()
        article.parse()
        content = article.text.strip()
        return content if len(content) >= CrawlerConfig.min_content_length else None
    except Exception:
        return None


# ============================================================
# ê¸°ì‚¬ ì„ íƒ í•¨ìˆ˜
# ============================================================
def select_articles(df: pd.DataFrame, num_select: int = 4) -> pd.DataFrame:
    """ì‚¬ìš©ìê°€ ê¸°ì‚¬ë¥¼ ì„ íƒí•  ìˆ˜ ìˆê²Œ í•¨"""
    if df.empty:
        print("ì„ íƒí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return df
    
    print(f"\n{'='*60}")
    print(f"ğŸ“° ì´ {len(df)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ - {num_select}ê°œë¥¼ ì„ íƒí•˜ì„¸ìš”")
    print(f"{'='*60}\n")
    
    display_df = df[["category", "company", "score", "title"]].copy()
    display_df.index = range(1, len(df) + 1)
    print(display_df.to_string())
    
    print(f"\nì„ íƒí•  ê¸°ì‚¬ ë²ˆí˜¸ {num_select}ê°œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„, ì˜ˆ: 5 6 3 15):")
    user_input = input(">>> ").strip()
    
    selected_indices = [int(x) for x in user_input.split()]
    selected_df = df.iloc[[i - 1 for i in selected_indices]].reset_index(drop=True)
    
    print(f"\nâœ… ì„ íƒ ì™„ë£Œ!")
    
    return selected_df


# ============================================================
# ë©”ì¸ í¬ë¡¤ëŸ¬
# ============================================================
def crawl_news(cfg: CrawlerConfig = CrawlerConfig()) -> pd.DataFrame:
    setup_ssl()
    
    article_config = Config()
    article_config.browser_user_agent = cfg.user_agent
    article_config.request_timeout = cfg.request_timeout

    results, seen_urls = [], set()
    
    print(f"ğŸ“… ìµœê·¼ {cfg.days}ì¼ ì´ë‚´ ë‰´ìŠ¤ ìˆ˜ì§‘")
    print(f"ğŸ“Œ ê¸°ì—…ë‹¹ 1ê°œ, ì´ {cfg.max_total}ê°œ ëª©í‘œ\n")
    
    for cat in SEARCH_CATEGORIES:
        if len(results) >= cfg.max_total:
            break
            
        print(f"\n{'='*50}\nğŸ“Œ [{cat['category']}] ê²€ìƒ‰ ì¤‘...")
        category_count = 0
        
        for company in cat["queries"]:
            if len(results) >= cfg.max_total:
                break
            
            print(f"\n  ğŸ” {company}")
            feed = feedparser.parse(get_rss_url(company, cfg.days))
            
            candidates = []
            for entry in feed.entries:
                if len(candidates) >= cfg.candidates_per_query:
                    break
                if any(kw in entry.title for kw in EXCLUDE_KEYWORDS):
                    continue
                
                url = decode_url(entry.link)
                if url in seen_urls:
                    continue
                
                content = fetch_article(url, article_config)
                if not content:
                    continue
                
                score = calculate_score(entry.title, content)
                candidates.append({
                    "category": cat["category"],
                    "company": company,
                    "title": entry.title,
                    "published": entry.published,
                    "link": url,
                    "content": content,
                    "score": score
                })
                print(f"    ğŸ“° {entry.title[:35]}... (ì ìˆ˜: {score})")
            
            if candidates:
                best = max(candidates, key=lambda x: x["score"])
                seen_urls.add(best["link"])
                results.append(best)
                category_count += 1
                print(f"    âœ… ì„ íƒ: {best['title'][:35]}...")
            else:
                print(f"    âš ï¸ ë‰´ìŠ¤ ì—†ìŒ")
        
        print(f"\n  ğŸ“Š {cat['category']}: {category_count}ê°œ")
    
    return pd.DataFrame(results)


def get_selected_news(num_select: int = 4) -> pd.DataFrame:
    """í¬ë¡¤ë§ í›„ ì‚¬ìš©ìê°€ ì„ íƒí•œ ê¸°ì‚¬ ë°˜í™˜"""
    df = crawl_news()
    
    if df.empty:
        return df
    
    return select_articles(df, num_select=num_select)


# í…ŒìŠ¤íŠ¸ìš© (ì§ì ‘ ì‹¤í–‰ ì‹œ)
if __name__ == "__main__":
    final_df = get_selected_news(num_select=4)
    
    if not final_df.empty:
        print(f"\n{'='*60}")
        print("ğŸ“‹ ìµœì¢… ì„ íƒëœ ê¸°ì‚¬:")
        print(f"{'='*60}\n")
        print(final_df[["category", "company", "score", "title"]].to_string())
