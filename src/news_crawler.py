import ssl
import urllib3
import logging
from dataclasses import dataclass
from typing import Optional

import feedparser
import pandas as pd
import requests
from googlenewsdecoder import gnewsdecoder
from newspaper import Article, Config
from .config import SELECTED_NEWS_FILE

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================
# Settings 
# ============================================================

# keywords that, if present in the title, will exclude the article
EXCLUDE_KEYWORDS = ["ë°°íƒ€ì ", "ì˜ìƒ", "ì¢…ëª©", "ì£¼ê°€", "ê¸‰ë“±", "ê¸‰ë½", "ë§¤ìˆ˜", "ë§¤ë„"]

# keywords with associated priority scores
PRIORITY_KEYWORDS = {
    "ì¶œì‹œ": 10, "ëŸ°ì¹­": 10, "ì˜¤í”ˆ": 8, "ì„œë¹„ìŠ¤": 12,
    "ë°œí‘œ": 6, "ë„ì…": 6, "ê°œë°œ": 5, "ìë™": 10,
    "ì±—ë´‡": 10, "GPT": 10, "ìƒì„±í˜•": 10, "LLM": 10,
    "í”Œë«í¼": 3, "ì†”ë£¨ì…˜": 3, "ì‹œìŠ¤í…œ": 2,
}

# To identify financial companies of a certain scale, the following companies were listed
# category: Industry of the company
# queries: List of company names to search for
SEARCH_CATEGORIES = [
    {
        "category": "ë³´í—˜ì‚¬",
        "queries": [
            "ì‚¼ì„±í™”ì¬", "í˜„ëŒ€í•´ìƒ", "DBì†í•´ë³´í—˜", "KBì†í•´ë³´í—˜", "ë©”ë¦¬ì¸ í™”ì¬", "í† ìŠ¤ì¸ìŠˆì–´ëŸ°ìŠ¤",
            "ì‚¼ì„±ìƒëª…", "êµë³´ìƒëª…", "í•œí™”ìƒëª…", "ì‹ í•œë¼ì´í”„", "NHë†í˜‘ìƒëª…", "KBë¼ì´í”„", "NHë†í˜‘ìƒëª…"
        ]
    },
    {
        "category": "ì€í–‰",
        "queries": ["í† ìŠ¤ë±…í¬", "ìš°ë¦¬ì€í–‰", "êµ­ë¯¼ì€í–‰", "ì‹ í•œì€í–‰", "í•˜ë‚˜ì€í–‰", "ê¸°ì—…ì€í–‰"]
    },
    {
        "category": "ì¹´ë“œì‚¬",
        "queries": ["ì‚¼ì„±ì¹´ë“œ", "ì‹ í•œì¹´ë“œ", "KBêµ­ë¯¼ì¹´ë“œ", "í˜„ëŒ€ì¹´ë“œ", "ë¡¯ë°ì¹´ë“œ", "ìš°ë¦¬ì¹´ë“œ", "í•˜ë‚˜ì¹´ë“œ", "BCì¹´ë“œ", "NHë†í˜‘ì¹´ë“œ"]
    },
    {
        "category": "Tech",
        "queries": ["êµ¬ê¸€", "OpenAI", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸"]
    },
    {
        "category": "ì¦ê¶Œì‚¬",
        "queries": ["NHíˆ¬ìì¦ê¶Œ", "ë¯¸ë˜ì—ì…‹ì¦ê¶Œ", "í•œêµ­íˆ¬ìì¦ê¶Œ", "ì‚¼ì„±ì¦ê¶Œ", "ì‹ í•œíˆ¬ìì¦ê¶Œ", "KBì¦ê¶Œ", "í‚¤ì›€ì¦ê¶Œ", "í† ìŠ¤ì¦ê¶Œ"]
    },
    {
        "category": "ê¸°íƒ€",
        "queries": ["ê¸ˆìœµ", "ì¸ê³µì§€ëŠ¥", "ê¸°í›„", "ììœ¨ì£¼í–‰", "ë³´í—˜"]
    },
]

# Calculate total number of companies from SEARCH_CATEGORIES
TOTAL_COMPANIES = sum(len(cat["queries"]) for cat in SEARCH_CATEGORIES)
# TOTAL_COMPANIES = 3 # for testing, limit to 3 companies (Use only when to debug)

@dataclass
class CrawlerConfig:
    max_total: int = TOTAL_COMPANIES # numbers of companies to crawl (auto-calculated from SEARCH_CATEGORIES)
    days: int = 14 # days to look back
    candidates_per_query: int = 5 # candidates per company query
    min_content_length: int = 150 # minimum length of article content
    request_timeout: int = 15 # seconds
    user_agent: str = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

# ============================================================
# SSL Settings
# ============================================================
def setup_ssl():
    ssl._create_default_https_context = ssl._create_unverified_context
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    original_request = requests.Session.request
    def patched_request(self, *args, **kwargs):
        kwargs['verify'] = False
        return original_request(self, *args, **kwargs)
    requests.Session.request = patched_request


# ============================================================
# Utility Functions
# ============================================================

# make rss url by company name and days to look for
def get_rss_url(query: str, days: int) -> str:
    encoded_query = f"{query} AI when:{days}d".replace(" ", "+")
    return f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"

# calculate article score based on presence of priority keywords
def calculate_score(title: str, content: str) -> int:
    text = f"{title} {content}"
    score = 0
    for keyword, weight in PRIORITY_KEYWORDS.items():
        if keyword in text:
            score += weight * 2 if keyword in title else weight
    return score

# RSS URLs are decoded into the original article URLs
def decode_url(link: str) -> str:
    try:
        result = gnewsdecoder(link)
        return result.get('decoded_url', link) if isinstance(result, dict) else result
    except Exception as e:
        logger.warning(f"Failed to decode URL {link}: {e}")
        return link

# Fetch Article Content
def fetch_article(url: str, config: Config) -> Optional[str]:
    try:
        article = Article(url, language='ko', config=config)
        article.download()
        article.parse()
        content = article.text.strip()
        return content if len(content) >= CrawlerConfig.min_content_length else None
    except Exception as e:
        logger.debug(f"Failed to fetch article {url}: {e}")
        return None


# ============================================================
# Select Articles (Human in the loop)
# ============================================================
def select_articles(df: pd.DataFrame, num_select: int = 4) -> pd.DataFrame:
    if df.empty:
        print("ì„ íƒí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return df
    
    print(f"\n{'='*60}")
    print(f"ğŸ“° ì´ {len(df)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ - {num_select}ê°œë¥¼ ì„ íƒí•˜ì„¸ìš”")
    print(f"{'='*60}\n")
    
    display_df = df[["category", "company", "score", "title"]].copy()
    display_df.index = range(1, len(df) + 1)
    print(display_df.to_string())

    print(f"\n[SELECT] ì„ íƒí•  ê¸°ì‚¬ ë²ˆí˜¸ {num_select}ê°œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„, ì˜ˆ: 5 6 3 15):")

    while True:
        try:
            user_input = input(">>> ").strip()

            if not user_input:
                print("âŒ ì…ë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue

            selected_indices = [int(x) for x in user_input.split()]

            # Validate that all indices are within valid range
            invalid_indices = [idx for idx in selected_indices if idx < 1 or idx > len(df)]
            if invalid_indices:
                print(f"âŒ ì˜ëª»ëœ ë²ˆí˜¸ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {invalid_indices}")
                print(f"   ìœ íš¨í•œ ë²”ìœ„: 1 ~ {len(df)}")
                continue

            if not selected_indices:
                print("âŒ ìµœì†Œ 1ê°œ ì´ìƒì˜ ê¸°ì‚¬ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
                continue

            # Validate number of selections
            if len(selected_indices) != num_select:
                print(f"âŒ {num_select}ê°œë¥¼ ì„ íƒí•´ì•¼ í•˜ì§€ë§Œ {len(selected_indices)}ê°œê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
                response = input(f"   ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                if response != 'y':
                    continue

            break

        except ValueError:
            print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ìˆ«ìë§Œ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì˜ˆ: 1 3 5)")
            continue

    selected_df = df.iloc[[i - 1 for i in selected_indices]].reset_index(drop=True)

    print(f"\nâœ… ì„ íƒ ì™„ë£Œ!")

    try:
        selected_df.to_excel(
            SELECTED_NEWS_FILE,
            index=False,
            engine='openpyxl'
        )
        print(f"ğŸ“ Excel ì €ì¥ ì™„ë£Œ: {SELECTED_NEWS_FILE}")
    except Exception as e:
        print(f"âŒ Excel íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        print("   ì„ íƒí•œ ë°ì´í„°ëŠ” ë©”ëª¨ë¦¬ì— ìœ ì§€ë˜ì§€ë§Œ íŒŒì¼ë¡œ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return selected_df


# ============================================================
# Main Crawler
# ============================================================
def crawl_news(cfg: CrawlerConfig = CrawlerConfig()) -> pd.DataFrame:
    setup_ssl()
    
    article_config = Config()
    article_config.browser_user_agent = cfg.user_agent
    article_config.request_timeout = cfg.request_timeout

    results, seen_urls = [], set()
    
    print(f"ğŸ“… ìµœê·¼ {cfg.days}ì¼ ì´ë‚´ ë‰´ìŠ¤ ìˆ˜ì§‘")
    print(f"ğŸ“Œ ê¸°ì—…ë‹¹ 1ê°œ, ì´ {cfg.max_total}ê°œ ëª©í‘œ\n")
    
    for cat in SEARCH_CATEGORIES:
        if len(results) >= cfg.max_total:
            break
            
        print(f"\n{'='*50}\nğŸ“Œ [{cat['category']}] ê²€ìƒ‰ ì¤‘...")
        category_count = 0
        
        for company in cat["queries"]:
            if len(results) >= cfg.max_total:
                break
            
            print(f"\n  ğŸ” {company}")
            # Article List Extraction
            feed = feedparser.parse(get_rss_url(company, cfg.days))
            
            candidates = []
            for entry in feed.entries:
                if len(candidates) >= cfg.candidates_per_query:
                    break
                # Articles are excluded if the title contains any EXCLUDE KEYWORDS
                if any(kw in entry.title for kw in EXCLUDE_KEYWORDS):
                    continue
                
                # Decode URL (RSS URL -> Original URL)
                url = decode_url(entry.link)
                if url in seen_urls:
                    continue
                
                # Fetch Article Content
                content = fetch_article(url, article_config)
                if not content:
                    continue
                
                #Calculate Score of Article
                score = calculate_score(entry.title, content)
                candidates.append({
                    "category": cat["category"],
                    "company": company,
                    "title": entry.title,
                    "published": entry.published,
                    "link": url,
                    "content": content,
                    "score": score
                })
                print(f"    ğŸ“° {entry.title[:35]}... (ì ìˆ˜: {score})")
            
            # Select the highest scored article among candidates
            if candidates:
                best = max(candidates, key=lambda x: x["score"])
                seen_urls.add(best["link"])
                results.append(best)
                category_count += 1
                print(f"    âœ… ì„ íƒ: {best['title'][:35]}...")
            else:
                print(f"    âš ï¸ ë‰´ìŠ¤ ì—†ìŒ")
        
        print(f"\n  ğŸ“Š {cat['category']}: {category_count}ê°œ")
    
    return pd.DataFrame(results)

# After crawling, return the articles selected by the user
def get_selected_news(num_select: int = 4) -> pd.DataFrame:
    df = crawl_news()
    
    if df.empty:
        return df
    
    return select_articles(df, num_select=num_select)


# Test (If needed)
if __name__ == "__main__":
    final_df = get_selected_news(num_select=4)
    
    if not final_df.empty:
        print(f"\n{'='*60}")
        print("ğŸ“‹ ìµœì¢… ì„ íƒëœ ê¸°ì‚¬:")
        print(f"{'='*60}\n")
        print(final_df[["category", "company", "score", "title"]].to_string())
