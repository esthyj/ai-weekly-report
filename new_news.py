import feedparser
from newspaper import Article, Config
import pandas as pd
from googlenewsdecoder import gnewsdecoder

def get_google_news_rss(query, days=14, lang="ko", country="KR"):
    query = f"{query} when:{days}d".replace(" ", "+")
    return f"https://news.google.com/rss/search?q={query}&hl={lang}&gl={country}&ceid={country}:{lang}"

def contains_exclude_keywords(text, exclude_keywords):
    """í…ìŠ¤íŠ¸ì— ì œì™¸ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    return any(keyword in text for keyword in exclude_keywords)

def calculate_priority_score(title, content):
    """ê¸°ì‚¬ì˜ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°"""
    
    # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œì™€ ê°€ì¤‘ì¹˜
    priority_keywords = {
        # ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ ì¶œì‹œ ê´€ë ¨ (ë†’ì€ ì ìˆ˜)
        "ì¶œì‹œ": 10,
        "ëŸ°ì¹­": 10,
        "ì˜¤í”ˆ": 8,
        "ì„œë¹„ìŠ¤": 12,
        "ë°œí‘œ": 6,
        "ë„ì…": 6,
        "ê°œë°œ": 5,
        "ìë™": 5,
        # AI ì„œë¹„ìŠ¤ ê´€ë ¨ (ì¤‘ê°„ ì ìˆ˜)
        "ì±—ë´‡": 10,
        "GPT": 10,
        "ìƒì„±í˜•": 10,
        "LLM": 10,
        "í”Œë«í¼": 3,
        "ì†”ë£¨ì…˜": 3,
        "ì‹œìŠ¤í…œ": 2,
    }
    
    score = 0
    text = title + " " + content
    
    for keyword, weight in priority_keywords.items():
        if keyword in text:
            # ì œëª©ì— ìˆìœ¼ë©´ ê°€ì¤‘ì¹˜ 2ë°°
            if keyword in title:
                score += weight * 2
            else:
                score += weight
    
    return score

def crawl_finance_ai_news(max_total=20, days=14, candidates_per_query=5):
    
    exclude_keywords = ["ë°°íƒ€ì ", "ì˜ìƒ", "ì¢…ëª©", "ì£¼ê°€", "ê¸‰ë“±", "ê¸‰ë½", "ë§¤ìˆ˜", "ë§¤ë„"]
    
    search_categories = [
        {
            "category": "ë³´í—˜ì‚¬",
            "priority": 1,
            "queries": [
                "ì‚¼ì„±í™”ì¬ AI",
                "ì‚¼ì„±ìƒëª… AI",
                "í˜„ëŒ€í•´ìƒ AI",
                "DBì†í•´ë³´í—˜ AI",
                "KBì†í•´ë³´í—˜ AI",
                "í•œí™”ìƒëª… AI",
                "ì‹ í•œë¼ì´í”„ AI",
            ]
        },
        {
            "category": "ì€í–‰",
            "priority": 2,
            "queries": [
                "ìš°ë¦¬ì€í–‰ AI",
                "êµ­ë¯¼ì€í–‰ AI",
                "ì‹ í•œì€í–‰ AI",
                "í•˜ë‚˜ì€í–‰ AI",
            ]
        },
        {
            "category": "í…Œí¬",
            "priority": 3,
            "queries": [
                "êµ¬ê¸€ AI",
                "OpenAI AI",
                "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ AI",
            ]
        },
        {
            "category": "ì¦ê¶Œì‚¬",
            "priority": 4,
            "queries": [
                "ë¯¸ë˜ì—ì…‹ì¦ê¶Œ AI",
                "í•œêµ­íˆ¬ìì¦ê¶Œ AI",
                "ì‚¼ì„±ì¦ê¶Œ AI",
            ]
        },
    ]
    
    config = Config()
    config.browser_user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    config.request_timeout = 15

    all_results = []
    seen_urls = set()
    
    print(f"ğŸ“… ìµœê·¼ {days}ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    print(f"ğŸ“Œ ê¸°ì—…ë‹¹ 1ê°œ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    print(f"â­ ìƒˆë¡œìš´ AI ì„œë¹„ìŠ¤ ì¶œì‹œ ê¸°ì‚¬ ìš°ì„  ì„ íƒ\n")
    
    for cat in search_categories:
        if len(all_results) >= max_total:
            break
            
        print(f"\n{'='*50}")
        print(f"ğŸ“Œ [{cat['category']}] ê²€ìƒ‰ ì¤‘... (ìš°ì„ ìˆœìœ„ {cat['priority']})")
        
        category_count = 0
        
        for query in cat["queries"]:
            if len(all_results) >= max_total:
                break
            
            print(f"\n  ğŸ” ê²€ìƒ‰: {query}")
            rss_url = get_google_news_rss(query, days=days)
            feed = feedparser.parse(rss_url)
            
            # í›„ë³´ ê¸°ì‚¬ë“¤ì„ ëª¨ì•„ì„œ ì ìˆ˜ ë¹„êµ
            candidates = []
            
            for entry in feed.entries:
                if len(candidates) >= candidates_per_query:
                    break
                
                # ì œëª©ì—ì„œ ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
                if contains_exclude_keywords(entry.title, exclude_keywords):
                    continue
                
                # URL ë””ì½”ë”©
                try:
                    decoded_res = gnewsdecoder(entry.link)
                    real_url = decoded_res.get('decoded_url', entry.link) if isinstance(decoded_res, dict) else decoded_res
                except:
                    real_url = entry.link
                
                if real_url in seen_urls:
                    continue
                
                # ê¸°ì‚¬ í¬ë¡¤ë§
                try:
                    article = Article(real_url, language='ko', config=config)
                    article.download()
                    article.parse()
                    
                    content = article.text.strip()
                    if len(content) < 150:
                        continue
                    
                    # ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
                    score = calculate_priority_score(entry.title, content)
                    
                    candidates.append({
                        "category": cat["category"],
                        "company": query.replace(" AI", ""),
                        "title": entry.title,
                        "published": entry.published,
                        "link": real_url,
                        "content": content,
                        "score": score
                    })
                    print(f"    ğŸ“° í›„ë³´: {entry.title[:35]}... (ì ìˆ˜: {score})")
                    
                except:
                    continue
            
            # ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ê¸°ì‚¬ ì„ íƒ
            if candidates:
                best_article = max(candidates, key=lambda x: x["score"])
                seen_urls.add(best_article["link"])
                all_results.append(best_article)
                category_count += 1
                print(f"    âœ… ì„ íƒ: {best_article['title'][:35]}... (ì ìˆ˜: {best_article['score']})")
            else:
                print(f"    âš ï¸ ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í•¨")
        
        print(f"\n  ğŸ“Š {cat['category']} ìˆ˜ì§‘: {category_count}ê°œ")
    
    return pd.DataFrame(all_results)


if __name__ == "__main__":
    df = crawl_finance_ai_news(max_total=20, days=14, candidates_per_query=5)
    
    if not df.empty:
        print(f"\n{'='*50}")
        print(f"ì´ {len(df)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ\n")
        
        # ì ìˆ˜ìˆœ ì •ë ¬í•´ì„œ ì¶œë ¥
        df_sorted = df.sort_values("score", ascending=False)
        print(df_sorted[["category", "company", "score", "title"]])